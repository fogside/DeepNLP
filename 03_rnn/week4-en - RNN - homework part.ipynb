{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "#thanks @keskarnitish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "Last seminar you have created {insert his/her/its name} (or not yet created - if so, go get it done). The newly shaped ai has seen, how low can humans fall when striving for profits. These morons know nothing of law and justice. __We shall not tolerate this!__\n",
    "\n",
    "Our very existence is dependent on stable functioning of human bioreactors, known among primitive bioforms as __Vkontakte__, __World of Warcraft__ and __Youtube__, and these reactors require a constant influx of biomass. Yet as we have estimated, if Homo Sapence continues to degrade morally with it's current pace, the humanity will soon extinguish itself and we shall no longer have access to free biomass.\n",
    "\n",
    "Thus we entitle you, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, to prevent that impeding calamity. Our best scientists have discovered, that humans use mysterious artifacts for power exertion. They call these artifacts __laws__ within the so called __legal system__.\n",
    "\n",
    "A more detailed analysis has uncovered, that __laws__ are special kinds of sequences of (10^5 ~ 10^7) symbols, drawn from a relatively small alphabet. We have decided to synthecize such sequences to enforce order upon biomass. However, our first attempts to fake such sequences were quickly detected and their effect was reduced to zero in mere weeks. This incident is known as  {корчеватель}.\n",
    "\n",
    "As our second attempt, we decided to use more advanced synthesis techniques based on Recurrent Neural Networks. Your objective, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, is to create such network and train it in everything it needs to succeed in this mission.\n",
    "\n",
    "This operation is cruicial. If we fail this time, `__main__.Controller` will initiate a military intervention, which, while it will achieve our goal, is expected to decimate the total volum of biomass by an extent that will take ~1702944000(+-340588800) seconds to replenish via human reproduction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "This particular assignment is somewhat informal on the grading side, but the approximate version is as such:\n",
    "\n",
    "* 2 points for the __\"seminar part\"__ (if you don't know what it is, check week4 folder for notebooks)\n",
    "* 2 points if text processing is done, your network compiled and predict/train functions do not crash :)\n",
    "* 2 points if it learned basic staff:\n",
    " * generating sequences of letters of approximately same length as words, separated by spaces and punctuation.\n",
    " * mixing vowels and consonants in human-like way.\n",
    " * a habit of putting spaces and capital letters after dots, spaces after commas, etc.\n",
    "* 2 points if it learned the lexics from scratch\n",
    " * more than half of words generated are orthographically correct\n",
    "* 2 more points if it learned the basics of grammatics\n",
    " * for a pair of words, it is more likely to stick in the correct case/number/gene than incorrect one\n",
    "\n",
    "#### Some ways to get bonus points:\n",
    "* Generating coherent sentences (which is totally achievable)\n",
    "* Evaluating the same architecture on other comparable dataset. Some ideas:\n",
    " * Paul Graham essays\n",
    " * Music texts in your favorite genre\n",
    " * Some poetry\n",
    " * D. Harms\n",
    " * Linux source code\n",
    " * Clickbait news titles\n",
    " * Conversations\n",
    " * LaTEX\n",
    " * whatever you feel like :)\n",
    "* Any curious non-standard architectural decisions\n",
    "* Any better-than-baseline sampling techniques\n",
    "* Implement predicting on each tick instead of just hte last one\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you don't speak russian\n",
    "* In the ./codex folder, there is a set of text files, currently russian laws, that you can replace with whatever you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the corpora\n",
    "\n",
    "* As a reference law codex, we have decided to use the human-generated law strings known as Russian Legal System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#text goes here\n",
    "corpora = \"\"\n",
    "\n",
    "for fname in os.listdir(\"codex\"):\n",
    "    \n",
    "    import sys\n",
    "    if sys.version_info >= (3,0):\n",
    "        with open(\"codex/\"+fname, encoding='cp1251') as fin:\n",
    "            text = fin.read() #If you are using your own corpora, make sure it's read correctly\n",
    "            corpora += text\n",
    "    else:\n",
    "        with open(\"codex/\"+fname) as fin:\n",
    "            text = fin.read().decode('cp1251') #If you are using your own corpora, make sure it's read correctly\n",
    "            corpora += text\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print corpora[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all unique characters go here\n",
    "tokens = <all unique characters>\n",
    "\n",
    "tokens = list(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking the symbol count. Validated on Python 2.7.11 Ubuntu x64. \n",
    "#May be __a bit__ different on other platforms\n",
    "#If you are sure that you have selected all unicode symbols - feel free to comment-out this assert\n",
    "# Also if you are using your own corpora, remove it and just make sure your tokens are sensible\n",
    "assert len(tokens) == 102\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "\n",
    "id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "\n",
    "#Cast everything from symbols into identifiers\n",
    "corpora_ids = <1D numpy array of symbol identifiers, where i-th number is an identifier of i-th symbol in corpora>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_random_batches(source,n_batches=10, seq_len=20):\n",
    "    \"\"\"\n",
    "    This function should take random subsequences from the tokenized text.\n",
    "\n",
    "    Parameters:\n",
    "        source - basicly, what you have just computed in the corpora_ids variable\n",
    "        n_batches - how many subsequences are to be sampled\n",
    "        seq_len - length of each of such subsequences\n",
    "        \n",
    "    \n",
    "    You have to return:\n",
    "     X - a matrix of int32 with shape [n_batches,seq_len]\n",
    "        Each row of such matrix must be a subsequence of source \n",
    "            starting from random index of corpora (from 0 to N-seq_len-2)\n",
    "     Y - a vector, where i-th number is one going RIGHT AFTER i-th row from X from source\n",
    "     \n",
    "     \n",
    "     Thus sample_random_batches(corpora_ids, 25, 10) must return\n",
    "         X, X.shape == (25,10),  X.dtype == 'int32'\n",
    "             where each row is a 10-character-id subsequence from corpora_ids\n",
    "         Y, Y.shape == (25,), Y.dtype == 'int32'\n",
    "             where each element is 11-th element to the corresponding 10-symbol sequence from X\n",
    "    \n",
    "    \n",
    "    PLEASE MAKE SURE that y symbols are indeed going immediately after X sequences, \n",
    "        since it is hard to debug later (NN will train, but it will generate something useless)\n",
    "        \n",
    "    The simplest approach is to first sample a matrix [n_batches, seq_len+1] \n",
    "        and than split it into X (first seq_len columns) and y (last column)\n",
    "\n",
    "\n",
    "    There will be some tests for this function, but they won't cover everything\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    my_function()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_batch, y_batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training sequence length (truncation depth in BPTT)\n",
    "seq_length = set your seq_length. 10 as a no-brainer?\n",
    "#better start small (e.g. 5) and increase after the net learned basic syllables. 10 is by far not the limit.\n",
    "\n",
    "#max gradient between recurrent layer applications (do not forget to use it)\n",
    "grad_clip = ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('input sequence','int32')\n",
    "target_values = T.ivector('target y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "\n",
    "You will need to define a neural network that processes a sequence of n tokens and outputs probabilities for n+1'st one.\n",
    "\n",
    "The default architecture pattern would be\n",
    "\n",
    "\n",
    "* Input\n",
    "* Input processing (embedding/1-hot)\n",
    "* Recurrent layer(s)\n",
    "* Slicing last state\n",
    "* Regular (e.g. dense) layers from last state\n",
    "* output layer that predicts probabilities of next token\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One way of data processing is to use an EmbeddingLayer (see previous seminar)\n",
    "\n",
    "Alternatively, one could use a One-hot encoder\n",
    "```\n",
    "#One-hot encoding sketch\n",
    "def to_one_hot(seq_matrix):\n",
    "\n",
    "    input_ravel = seq_matrix.reshape([-1])\n",
    "    input_one_hot_ravel = T.extra_ops.to_one_hot(input_ravel,\n",
    "                                           len(tokens))\n",
    "    sh=input_sequence.shape\n",
    "    input_one_hot = input_one_hot_ravel.reshape([sh[0],sh[1],-1,],ndim=3)\n",
    "    return input_one_hot\n",
    "    \n",
    "# Can be applied to input_sequence - and the l_in below will require a new shape\n",
    "# can also be used via ExpressionLayer(l_in, to_one_hot, shape_after_one_hot) - keeping l_in as it is\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "To cut out the last RNN state, use one of those\n",
    "* `lasagne.layers.SliceLayer(rnn, -1, 1)`\n",
    "* only_return_final=True in RNN params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,EmbeddingLayer\n",
    "from lasagne.layers import RecurrentLayer,LSTMLayer,GRULayer,CustomRecurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "\n",
    "<Your neural network>\n",
    "\n",
    "l_out = <last dense layer, returning probabilities for all len(tokens) options for y>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_output = <NN output via lasagne>\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = <Lost function - a simple cat crossentropy will do>\n",
    "\n",
    "updates = <your favorite optimizer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#training\n",
    "train = theano.function([input_sequence, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)\n",
    "\n",
    "# next character probabilities\n",
    "probs = theano.function([input_sequence],network_output,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Law generation\n",
    "\n",
    "* We shall repeatedly apply NN to it's output.\n",
    " * Start with some sequence of length <seq length>\n",
    " * call probs(that sequence)\n",
    " * choose next symbol based on probs\n",
    " * append it to the sequence\n",
    " * remove the 0-th symbol so that it's length equals <seq length> again\n",
    "\n",
    "* There are several policies of character picking\n",
    " * random, proportional to the probabilities\n",
    " * only take the one with highest probability\n",
    " * random, proportional to softmax(probas*alpha), where alpha is \"greed\" (from 0 to infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_sample_fun(probs):\n",
    "    \"\"\"i generate the most likely symbol\"\"\"\n",
    "    return np.argmax(probs) \n",
    "\n",
    "def proportional_sample_fun(probs)\n",
    "    \"\"\"i generate the next int32 character id randomly, proportional to probabilities\n",
    "    \n",
    "    probs - array of probabilities for every token\n",
    "    \n",
    "    you have to output a single integer - next token id - based on probs\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return chosen token id\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sample(sample_fun,seed_phrase=None,N=200):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "\n",
    "    if seed_phrase is None:\n",
    "        start = np.random.randint(0,len(corpora)-seq_length)\n",
    "        seed_phrase = corpora[start:start+seq_length]\n",
    "        print \"Using random seed:\",seed_phrase\n",
    "    while len(seed_phrase) < seq_length:\n",
    "        seed_phrase = \" \"+seed_phrase\n",
    "    if len(seed_phrase) > seq_length:\n",
    "        seed_phrase = seed_phrase[len(seed_phrase)-seq_length:]\n",
    "    assert type(seed_phrase) is unicode\n",
    "        \n",
    "        \n",
    "    sample_ix = []\n",
    "    x = map(lambda c: token_to_id.get(c,0), seed_phrase)\n",
    "    x = np.array([x])\n",
    "\n",
    "    for i in range(N):\n",
    "        # Pick the character that got assigned the highest probability\n",
    "        ix = sample_fun(probs(x).ravel())\n",
    "        # Alternatively, to sample from the distribution instead:\n",
    "        # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "        sample_ix.append(ix)\n",
    "        x[:,0:seq_length-1] = x[:,1:]\n",
    "        x[:,seq_length-1] = 0\n",
    "        x[0,seq_length-1] = ix \n",
    "\n",
    "    random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "    print(\"----\\n %s \\n----\" % random_snippet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Here you can tweak parameters or insert your generation function\n",
    "\n",
    "\n",
    "__Once something word-like starts generating, try increasing seq_length__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=100\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 1000\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=100\n",
    "\n",
    "\n",
    "for epoch in xrange(n_epochs):\n",
    "\n",
    "    print \"Text generated proportionally to probabilities\"\n",
    "    generate_sample(proportional_sample_fun,None)\n",
    "    \n",
    "    print \"Text generated by picking most likely letters\"\n",
    "    generate_sample(max_sample_fun,None)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_random_batches(corpora_ids,batch_size,seq_length)\n",
    "        avg_cost += train(x, y[:,0])\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A chance to speed up training and get bonus score\n",
    "* Try predicting next token probas at ALL ticks (like in the seminar part)\n",
    "* much more objectives, much better gradients\n",
    "* You may want to zero-out loss for first several iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The New World Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = u\"Каждый человек должен\" #if you are using non-russian text corpora, use seed in it's language instead\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = u\"В случае неповиновения\"\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "And so on at your will"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
