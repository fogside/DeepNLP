{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "#thanks @keskarnitish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate names\n",
    "* Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train NN instead.\n",
    "* Dataset contains ~8k human names from different cultures[in latin transcript]\n",
    "* Objective (toy problem): learn a generative model over names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      " Abagael\n",
      " Claresta\n",
      " Glory\n",
      " Liliane\n",
      " Prissie\n",
      " Geeta\n",
      " Giovanne\n",
      " Piggy\n"
     ]
    }
   ],
   "source": [
    "print 'n samples = ',len(names)\n",
    "for x in names[::1000]:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  55\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "token_set = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        token_set.add(letter)\n",
    "\n",
    "tokens = list(token_set)\n",
    "\n",
    "print 'n_tokens = ',len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "token_to_id = {t:i for i,t in enumerate(tokens) }\n",
    "\n",
    "#!id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "id_to_token = {i:t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6TGGPyS1Jj/AyU4/MHunlgAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc81d9a0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(map(len,names),bins=25);\n",
    "\n",
    "# truncate names longer than MAX_LEN characters. \n",
    "MAX_LEN = min([60,max(list(map(len,names)))])\n",
    "#ADJUST IF YOU ARE UP TO SOMETHING SERIOUS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get,name)),names))\n",
    "\n",
    "\n",
    "#crop long names and pad short ones\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len,names_ix)))==1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequencea','int32')\n",
    "target_values = T.matrix('actual next token','int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN\n",
    "\n",
    "You will be building a model that takes token sequence and predicts next token\n",
    "\n",
    "\n",
    "* iput sequence\n",
    "* one-hot / embedding\n",
    "* recurrent layer(s)\n",
    "* otput layer(s) that predict output probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,EmbeddingLayer\n",
    "from lasagne.layers import RecurrentLayer,LSTMLayer,GRULayer,CustomRecurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "\n",
    "#!<Your neural network>\n",
    "l_emb = lasagne.layers.EmbeddingLayer(l_in, len(tokens), 40)\n",
    "\n",
    "l_rnn = lasagne.layers.RecurrentLayer(l_emb,40,nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "l_rnn_flat = lasagne.layers.reshape(l_rnn, (-1,l_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(l_rnn_flat,len(tokens), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, input_to_hidden.W, input_to_hidden.b, hidden_to_hidden.W, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_probabilities_flat = network_output\n",
    "correct_answers_flat = target_values.ravel()\n",
    "\n",
    "\n",
    "loss = T.mean(lasagne.objectives.categorical_crossentropy(predicted_probabilities_flat, correct_answers_flat))\n",
    "#<Loss function - a simple categorical crossentropy will do, maybe add some regularizer>\n",
    "\n",
    "updates = lasagne.updates.adam(loss,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#training\n",
    "train = theano.function([input_sequence, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation\n",
    "\n",
    "Simple: \n",
    "* get initial context(seed), \n",
    "* predict next token probabilities,\n",
    "* sample next token, \n",
    "* add it to the context\n",
    "* repeat from step 2\n",
    "\n",
    "You'll get a more detailed info on how it works in the homework section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "#reshape back into original shape\n",
    "next_word_probas = network_output.reshape((input_sequence.shape[0],input_sequence.shape[1],len(tokens)))\n",
    "#predictions for next tokens (after sequence end)\n",
    "last_word_probas = next_word_probas[:,-1]\n",
    "probs = theano.function([input_sequence],last_word_probas,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sample(seed_phrase=None,N=MAX_LEN,t=1,n_snippets=1):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        seed_phrase=start_token\n",
    "    if len(seed_phrase) > MAX_LEN:\n",
    "        seed_phrase = seed_phrase[-MAX_LEN:]\n",
    "    assert type(seed_phrase) is str\n",
    "\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        sample_ix = []\n",
    "        x = map(lambda c: token_to_id.get(c,0), seed_phrase)\n",
    "        x = np.array([x])\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            p = probs(x).ravel()\n",
    "            p = p**t / np.sum(p**t)\n",
    "            ix = np.random.choice(np.arange(len(tokens)),p=p)\n",
    "            sample_ix.append(ix)\n",
    "\n",
    "            x = np.hstack((x[-MAX_LEN+1:],[[ix]]))\n",
    "\n",
    "        random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "        snippets.append(random_snippet)\n",
    "        \n",
    "    print(\"----\\n %s \\n----\" % '; '.join(snippets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Here you can tweak parameters or insert your generation function\n",
    "\n",
    "\n",
    "__Once something word-like starts generating, try increasing seq_length__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    \n",
    "    return rows[:,:-1],rows[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Generated names\n",
      "----\n",
      "  SokPmyFAqxCVPYdQ;  OUzNwDZOcrfCmk T;  gCVhDeezdNwpa AJ;  tLoAyyPmatZXmNJP;  JYFAvkWkaY'FNLAi;  od KeXCPJKlWwIAo;  sEyNBrYrQIXesgfP;  Evbjvnnj'B'oOUmj;  FNYDxgPGpHxNaM'h;  IUm'Y-Cn'm-jOZiF \n",
      "----\n",
      "Epoch 0 average loss = 1.56753074179\n",
      "Generated names\n",
      "----\n",
      "  Eiiica          ;  Aoy             ;  ZTdae           ;  Gabla           ;  SJdtorati       ;  Kdnnannnti      ;  wgudng          ;  Paaoitk         ;  Debeo           ;  Qcar             \n",
      "----\n",
      "Epoch 1 average loss = 1.18419929487\n",
      "Generated names\n",
      "----\n",
      "  Lana            ;  Wonnbe          ;  Bormhae         ;  Kmisia          ;  Pavery          ;  Nychana         ;  Ititine         ;  Giljo           ;  Donadca         ;  Modcara          \n",
      "----\n",
      "Epoch 2 average loss = 1.12238639395\n",
      "Generated names\n",
      "----\n",
      "  Tasmi           ;  Nouil           ;  Folbene         ;  Rilnednar       ;  Lanne           ;  Jicalenen       ;  Toencid         ;  Idy             ;  Vamanal         ;  Groliro          \n",
      "----\n",
      "Epoch 3 average loss = 1.098761891\n",
      "Generated names\n",
      "----\n",
      "  Kardi           ;  Dara            ;  Eliin           ;  Parod           ;  Garriata        ;  Meri-hra        ;  Pevioo          ;  Posrr           ;  Torshinta       ;  Bawie            \n",
      "----\n",
      "Epoch 4 average loss = 1.08885626313\n",
      "Generated names\n",
      "----\n",
      "  Wynkr           ;  Ludna           ;  Rox             ;  Reenicae        ;  Tiuna           ;  Flizgie         ;  Kellio          ;  Canhre          ;  Tutte           ;  Leriethy         \n",
      "----\n",
      "Epoch 5 average loss = 1.06321364653\n",
      "Generated names\n",
      "----\n",
      "  Kiybe           ;  Rirlen          ;  Frig            ;  Tlaras          ;  Mawhela         ;  Pistiil         ;  Ractel          ;  Ganceblana      ;  Tlanne          ;  Kitia            \n",
      "----\n",
      "Epoch 6 average loss = 1.04974379667\n",
      "Generated names\n",
      "----\n",
      "  Muley           ;  Jeddulie        ;  Wak             ;  Atris           ;  Binnany         ;  Flyn            ;  Shrinista       ;  Comrann         ;  Nonda           ;  Nas              \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f216df1f70fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_ix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch {} average loss = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_cost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=100\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=10\n",
    "\n",
    "\n",
    "for epoch in xrange(n_epochs):\n",
    "\n",
    "    print \"Generated names\"\n",
    "    generate_sample(n_snippets=10)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_batch(names_ix,batch_size)\n",
    "        avg_cost += train(x, y)\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  Janalla         ;  Thad            ;  Ilondie         ;  Chuan           ;  Cinelia         ;  Marelne         ;  Cin             ;  Alie            ;  Jacie           ;  Merrie           \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(n_snippets=10,t=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  Putinder             ;  Putina               ;  Putine               ;  Putina               ;  Putinnp              ;  Putine               ;  Putinella            ;  Putina               ;  Putino               ;  Putini               ;  Putina               ;  Putino               ;  Putins               ;  Putinie              ;  Putine               ;  Putina               ;  Putinas              ;  Putine               ;  Putin              n ;  Putinc               ;  Putin                ;  Putiniche            ;  Putinel              ;  Putinha              ;  Putin                ;  Putindie             ;  Putin                ;  Putinne              ;  Putinconnin          ;  Putinae              ;  Putinsh              ;  Putina               ;  Putink               ;  Putin                ;  Putinie              ;  Putine               ;  Putinsy              ;  Putin                ;  Putine               ;  Putine               ;  Putincons            ;  Putin                ;  Putine               ;  Putinie              ;  Putini               ;  Putinel              ;  Putin                ;  Putine               ;  Putina               ;  Putine               ;  Putine               ;  Putiniana            ;  Putine               ;  Putine               ;  Putinat              ;  Putina               ;  Putina               ;  Putinca              ;  Putina               ;  Putine               ;  Putin                ;  Putini               ;  Putinila             ;  Putin                ;  Putinitsana          ;  Putine               ;  Putin                ;  Putine               ;  Putinre              ;  Putinem              ;  Putine               ;  Putin                ;  Putinead             ;  Putin                ;  Putine               ;  Putin                ;  Putinka              ;  Putin                ;  Putine               ;  Putinen              ;  Putina               ;  Putinha              ;  Putinon              ;  Putine               ;  Putine               ;  Putine               ;  Putinsta             ;  Putin                ;  Putind               ;  Putinen              ;  Putina               ;  Putined              ;  Putino               ;  Putiney              ;  Putina               ;  Putinef              ;  Putina               ;  Putinay              ;  Putine               ;  Putiny                \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(seed_phrase=\" Putin\",n_snippets=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now,\n",
    "* try lstm/gru\n",
    "* try several layers\n",
    "* try mtg cards\n",
    "* try your own dataset of any kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
