{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "DeepLarning Couse HSE 2016 fall: \n",
    "* Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```,\n",
    "* ```https://vk.com/ars.ashuha``` \n",
    "* partially reusing https://github.com/ebenolson/pydata2015\n",
    "\n",
    "<h1 align=\"center\"> Image Captioning </h1> \n",
    "\n",
    "In this seminar you'll be going through the image captioning pipeline.\n",
    "\n",
    "To begin with, let us download the dataset of image features from a pre-trained GoogleNet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Or alternatevely\n",
    "### !wget https://www.dropbox.com/s/3hj16b0fj6yw7cc/data.tar.gz?dl=1 -O data.tar.gz\n",
    "### !tar -xvzf data.tar.gz\n",
    "###\n",
    "\n",
    "DATA_DIR = '/wrk/trng103/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read Dataset\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "img_codes = np.load(osp.join(DATA_DIR, \"data/image_codes.npy\"))\n",
    "captions = pickle.load(open(osp.join(DATA_DIR, 'data/caption_tokens.pcl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (\"each image code is a 1000-unit vector:\", img_codes.shape)\n",
    "print (img_codes[0,:10])\n",
    "print ('\\n\\n')\n",
    "print (\"for each image there are 5-7 descriptions, e.g.:\\n\")\n",
    "print ('\\n'.join(captions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"]+sentence.split(' ')+[\"#END#\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "\n",
    "<Compute word frequencies for each word in captions. See code above for data structure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PAD_ix = -1\n",
    "UNK_ix = vocab.index('#UNK#')\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences,max_len=None):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int32')+PAD_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word,UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mah Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# network shapes. \n",
    "CNN_FEATURE_SIZE = img_codes.shape[1]\n",
    "EMBED_SIZE = 128 #pls change me if u want\n",
    "LSTM_UNITS = 200 #pls change me if u want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input Variable\n",
    "sentences = T.imatrix()# [batch_size x time] of word ids\n",
    "image_vectors = T.matrix() # [batch size x unit] of CNN image features\n",
    "sentence_mask = T.neq(sentences,PAD_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#network inputs\n",
    "l_words = InputLayer((None,None),sentences )\n",
    "l_mask = InputLayer((None,None),sentence_mask )\n",
    "\n",
    "#embeddings for words \n",
    "l_word_embeddings = <apply word embedding. use EMBED_SIZE>\n",
    "\n",
    "#cudos for using some pre-trained embedding :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input layer for image features\n",
    "l_image_features = InputLayer((None,CNN_FEATURE_SIZE),image_vectors )\n",
    "\n",
    "#convert 1000 image features from googlenet to whatever LSTM_UNITS you have set\n",
    "#it's also a good idea to add some dropout here and there\n",
    "l_image_features_small = <convert l_image features to a shape equal to rnn hidden state. Also play with dropout/noize>\n",
    "\n",
    "assert l_image_features_small.output_shape == (None,LSTM_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Concatinate image features and word embedings in one sequence \n",
    "decoder = a recurrent layer (gru/lstm) with following checklist:\n",
    "#    * takes word embeddings as an input\n",
    "#    * has LSTM_UNITS units in the final layer\n",
    "#    * has cell_init (or hid init for gru) set to converted image features\n",
    "#    * mask_input = input_mask\n",
    "#    * don't forget the grad clipping (~5-10)\n",
    "\n",
    "#find out better recurrent architectures for bonus point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decoding of rnn hiden states\n",
    "from broadcast import BroadcastLayer,UnbroadcastLayer\n",
    "\n",
    "#apply whatever comes next to each tick of each example in a batch. Equivalent to 2 reshapes\n",
    "broadcast_decoder_ticks = BroadcastLayer(decoder,(0,1))\n",
    "print \"broadcasted decoder shape = \",broadcast_decoder_ticks.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#predict probabilities for next tokens\n",
    "predicted_probabilities_each_tick = <predict probabilities for each tick, using broadcasted_decoder_shape as an input. No reshaping needed here.>\n",
    "# maybe a more complicated architecture will work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#un-broadcast back into (batch,tick,probabilities)\n",
    "predicted_probabilities = UnbroadcastLayer(predicted_probabilities_each_tick,\n",
    "                                           broadcast_layer=broadcast_decoder_ticks)\n",
    "\n",
    "print \"output shape = \",predicted_probabilities.output_shape\n",
    "\n",
    "#remove if you know what you're doing (e.g. 1d convolutions or fixed shape)\n",
    "assert predicted_probabilities.output_shape == (None, None, 10373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some tricks\n",
    "* If you train large network, it is usually a good idea to make a 2-stage prediction\n",
    "    1. (large recurrent state) -> (bottleneck e.g. 256)\n",
    "    2. (bottleneck) -> (vocabulary size)\n",
    "    * this way you won't need to store/train (large_recurrent_state x vocabulary size) matrix\n",
    "* Also maybe use Hierarchical Softmax?\n",
    "    * https://gist.github.com/justheuristic/581853c6d6b87eae9669297c2fb1052d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "next_word_probas = <get network output>\n",
    "\n",
    "\n",
    "predictions_flat = next_word_probas[:,:-1].reshape((-1,n_tokens))\n",
    "reference_answers = sentences[:,1:].reshape((-1,))\n",
    "\n",
    "#write symbolic loss function to train NN for\n",
    "loss = <compute elementwise loss function>\n",
    "\n",
    "#mean over non-PAD tokens\n",
    "output_mask = sentence_mask[:,1:]\n",
    "loss = (loss.reshape(reference_answers.shape)*output_mask).sum() / output_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#trainable NN weights\n",
    "weights = get_all_params(predicted_probabilities,trainable=True)\n",
    "updates = <parameter updates using your favorite algoritm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#compile a functions for training and evaluation\n",
    "#please not that your functions must accept image features as FIRST param and sentences as second one\n",
    "train_step = <function that takes input sentence and image mask, outputs loss and updates weights>\n",
    "val_step   = <function that takes input sentence and image mask and outputs loss>\n",
    "#for val_step use deterministic=True if you have any dropout/noize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training\n",
    "\n",
    "* You first have to implement a batch generator\n",
    "* Than the network will get trained the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "captions = np.array(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(images,captions,batch_size,max_caption_len=None):\n",
    "    \n",
    "    #sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0,len(images),size=batch_size)\n",
    "    \n",
    "    #get images\n",
    "    batch_images = images[random_image_ix]\n",
    "    \n",
    "    #5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    #pick 1 from 5-7 captions for each image\n",
    "    batch_captions = map(choice,captions_for_batch_images)\n",
    "    \n",
    "    #convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return batch_images, batch_captions_ix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generate_batch(img_codes,captions,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main loop\n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    " *  its safe to interrupt training, run a few examples and start training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size=50 #adjust me\n",
    "n_epochs=100 #adjust me\n",
    "n_batches_per_epoch = 50 #adjust me\n",
    "n_validation_batches = 5 #how many batches are used for validation after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip3 install --user tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss=0\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        train_loss += train_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss=0\n",
    "    for _ in range(n_validation_batches):\n",
    "        val_loss += val_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "print(\"Finish :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### apply trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip3 install --user scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the same kind you did last week, but a bit smaller\n",
    "from pretrained_lenet import build_model,preprocess,MEAN_VALUES\n",
    "\n",
    "# build googlenet\n",
    "lenet = build_model()\n",
    "\n",
    "#load weights\n",
    "#lenet_weights = pickle.load(open(osp.join(DATA_DIR, 'data/blvc_googlenet.pkl')), encoding='latin1')['param values']\n",
    "lenet_weights = np.load(osp.join(DATA_DIR, 'data/blvc_googlenet.npz'), encoding='latin1')['param values']\n",
    "set_all_param_values(lenet[\"prob\"], lenet_weights)\n",
    "\n",
    "#compile get_features\n",
    "cnn_input_var = lenet['input'].input_var\n",
    "cnn_feature_layer = lenet['loss3/classifier']\n",
    "get_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sample image\n",
    "img = plt.imread(osp.join(DATA_DIR, 'data/Dog-and-Cat.jpg'))\n",
    "img = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#deprocess and show, one line :)\n",
    "from pretrained_lenet import MEAN_VALUES\n",
    "plt.imshow(np.transpose((img[0] + MEAN_VALUES)[::-1],[1,2,0]).astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_word_probas = <get network-predicted probas at last tick\n",
    "#TRY OUT deterministic=True if you want more steady results\n",
    "\n",
    "get_probs = theano.function([image_vectors,sentences], last_word_probas)\n",
    "\n",
    "#this is exactly the generation function from week5 classwork,\n",
    "#except now we condition on image features instead of words\n",
    "def generate_caption(image,caption_prefix = (\"START\",),t=1,sample=True,max_len=100):\n",
    "    image_features = get_cnn_features(image)\n",
    "    caption = list(caption_prefix)\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        next_word_probs = <obtain probabilities for next words>\n",
    "        assert len(next_word_probs.shape) ==1 #must be one-dimensional\n",
    "        #apply temperature\n",
    "        next_word_probs = next_word_probs**t / np.sum(next_word_probs**t)\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab,p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption.append(next_word)\n",
    "\n",
    "        if next_word==\"#END#\":\n",
    "            break\n",
    "            \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print ' '.join(generate_caption(img,t=5.)[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Demo\n",
    "### Find at least 10 images to test it on.\n",
    "* Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n",
    "* Make sure it works okay on __simple__ images before going to something more comples\n",
    "* Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n",
    "* Mind the aspect ratio (see what `preprocess` does to your image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#apply your network on image sample you found\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# grading\n",
    "\n",
    "* base 5 if it compiles and trains without exploding\n",
    "* +1 for finding representative set of reference examples\n",
    "* +2 for providing 10+ examples where network provides reasonable captions (at least sometimes :) )\n",
    "  * you may want to predict with sample=False and deterministic=True for consistent results\n",
    "  * kudos for submitting network params that reproduce it\n",
    "* +2 for providing 10+ examples where network fails IF you also got previous 10 examples right\n",
    "\n",
    "\n",
    "* bonus points for experiments with architecture and initialization (see above)\n",
    "* bonus points for trying out other pre-trained nets for captioning\n",
    "* a whole lot of bonus points if you also train via metric learning\n",
    " * image -> vec\n",
    " * caption -> vec (encoder, not decoder)\n",
    " * loss = correct captions must be closer, wrong ones must be farther\n",
    " * prediction = choose caption that is closest to image\n",
    "* a freaking whole lot of points if you also obtain statistically signifficant results the other way round\n",
    " * take caption, get closest image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
