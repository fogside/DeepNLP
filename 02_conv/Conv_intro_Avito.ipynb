{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Сегодня мы поговорим о сверточных нейросетях.\n",
    "\n",
    "Сначала поймем суть _`convolutions`_, а затем попробуем решить реальную задачу по классификации объявлений на _Avito_.\n",
    "\n",
    "Если вы смотрите этот ноутбук, то вероятно ваш семинарист где-то рядом и как раз объясняет что к чему.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------- Just imports to make everything ok --------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Для красивого вывода графиков и картиночек\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 30\n",
    "\n",
    "# Если хотите можете\n",
    "# написать тут более подходящее место\n",
    "# для сохранения картиночек\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# Еще пара вспомогательных функций, чтобы красиво отображать grayscale & RGB\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И конечно tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что такое свертки и с чем их едят\n",
    "\n",
    "\n",
    "Ожидается, что на данном этапе семинарист объяснил вам у доски что же такое свертки.\n",
    "\n",
    "Прилагается немного картинок.\n",
    "\n",
    "Про `padding` -- \"SAME\" & \"VALID\" обозначения использующиеся в tf, Keras и мб где-то еще.\n",
    "* \"SAME\" -- это добивание нулями входной матрицы так, чтобы возможно было учесть в свертке все пиксели.\n",
    "* \"VALID\" -- просто игнорируем те пиксели, которые не попадают в свертку. Нулей не добавляем.\n",
    "\n",
    "<img src=\"./pics/padding.png\" width=\"50%\">\n",
    "\n",
    "__strides__ -- это массив параметров, указывающих на сколько мы сдвигаемся по горизонтали и вертикали при пробегании входной матрицы сверткой.\n",
    "<img src=\"./pics/strides.png\" width=\"50%\">\n",
    "\n",
    "Картинка, чтобы было понятно, что один сверточный слой состоит из кучи различных карт фильтров.\n",
    "А каждый нейрон из $n+1$ слоя связан со всеми подслоями из $n$-ого слоя.\n",
    "\n",
    "<img src=\"./pics/deep_conv.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "image = china[150:220, 130:250]\n",
    "height, width, channels = image.shape\n",
    "image_grayscale = image.mean(axis=2).astype(np.float32)\n",
    "images = image_grayscale.reshape(1, height, width, 1)\n",
    "print(\"Это фильтры, которые мы будем накладывать с помощью сверток:\")\n",
    "\n",
    "\n",
    "fmap = np.zeros(shape=(7, 7, 1, 2), dtype=np.float32)\n",
    "fmap[:, 3, 0, 0] = 1\n",
    "fmap[3, :, 0, 1] = 1\n",
    "fmap[:, :, 0, 0]\n",
    "plot_image(fmap[:, :, 0, 0])\n",
    "plt.show()\n",
    "plot_image(fmap[:, :, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, 1))\n",
    "feature_maps = tf.constant(fmap)\n",
    "convolution = tf.nn.conv2d(X, feature_maps, strides=[1,1,1,1], padding=\"SAME\", use_cudnn_on_gpu=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = convolution.eval(feed_dict={X: images})\n",
    "\n",
    "    \n",
    "# Отобразим результат\n",
    "\n",
    "fig = plt.figure(figsize=(36, 12))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plot_image(images[0, :, :, 0])\n",
    "ax.set_title(\"china_original\", )\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plot_image(output[0, :, :, 0])\n",
    "ax.set_title(\"china_vertical\")\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "plot_image(output[0, :, :, 1])\n",
    "ax.set_title(\"china_horizontal\")\n",
    "ax.grid('off')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим что будет, если наложить сразу 2 таких фильтра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.array([china, flower], dtype=np.float32)\n",
    "# dataset = np.array([image], dtype=np.float32)\n",
    "batch_size, height, width, channels = dataset.shape\n",
    "\n",
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "# Create a graph with input X plus a convolutional layer applying the 2 filters\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(convolution, feed_dict={X: dataset})\n",
    "    \n",
    "fig = plt.figure(figsize=(20, 13))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "plot_image(output[0, :, :, 1]) # Второй feature-map первой картинки\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "plot_image(output[0, 110:160, 45:135, 1])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "plot_image(output[1, :, :, 1]) # Второй feature-map второй картинки\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В tf.layers.conv2d веса фильтров первоначально инициализируются рандомно путем сэмплирования из равномерного распределения (glorot uniform)\n",
    "\n",
    "Посмотрим как будет выглядеть картинка, если к ней сразу применить такие фильтры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(shape=(None, height, width, channels), dtype=tf.float32)\n",
    "conv = tf.layers.conv2d(X, filters=2, kernel_size=7, strides=[2,2],\n",
    "                        padding=\"SAME\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    output = sess.run(conv, feed_dict={X: dataset})\n",
    "    \n",
    "plt.imshow(output[0, :, :, 1], cmap=\"gray\") # Второй feature-map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Max pooling / Average pooling\n",
    "\n",
    "<img src=\"./pics/maxPool.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "\n",
    "# strides=[1,2,2,1] -- означает, что pooling делается как на картинке сверху\n",
    "max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "avg_pool = tf.nn.avg_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output_m, output_avg = sess.run([max_pool, avg_pool], feed_dict={X: dataset})\n",
    "\n",
    "fig = plt.figure(figsize=(36, 12))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plot_color_image(dataset[0])\n",
    "ax.set_title(\"Original shape: {}\".format(dataset[0].shape))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plot_color_image(output_m[0])\n",
    "ax.set_title(\"Max pooled shape: {}\".format(output_m[0].shape))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "plot_color_image(output_avg[0])\n",
    "ax.set_title(\"Avg pooled shape: {}\".format(output_avg[0].shape))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito challenge\n",
    "\n",
    "Конечно, сама идея сверточных нейросетей исходит из анализа работы органов зрения. Поэтому не удивительно, что свертки отлично справляются с анализом изображений. Но их также можно применять для анализа последовательностей. Для этого есть одномерные свертки. И они тоже очень неплохо работают.\n",
    "\n",
    "Сейчас предлагается опробовать свои силы в решении реальной задачи по классификации объявлений на Avito.\n",
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Познакомимся с данными\n",
    "\n",
    "Бывший kaggle-конкурс про выявление нежелательного контента.\n",
    "\n",
    "Описание конкурса есть тут - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "Сегодня мы будем пользоваться прореженными train данными чтобы даже те, у кого небольшой RAM (~4Gb) смогли нормально работать.\n",
    "Оригинальные данные все еще доступны для скачивания на Kaggle.\n",
    " \n",
    "Много разных признаков:\n",
    "* 2 вида текста - заголовок и описание\n",
    "* Много специальных фичей - цена, количество телефонов/ссылок/e-mail адресов\n",
    "* Категория и субкатегория - как ни странно, категориальные фичи\n",
    "* Аттрибуты - много категориальных признаков\n",
    "\n",
    "Нужно предсказать всего 1 бинарный признак - есть ли в рекламе нежелательный контент.\n",
    "* Под нежелательным контентом понимается криминал, прон, афера, треска и прочие любимые нами темы.\n",
    "* Да, если присмотреться к заблокированным объявлениям, можно потерять аппетит и сон на пару дней.\n",
    "* Однако профессия аналитика данных обязывает вас смотреть на данные.\n",
    " * А кто сказал, что будет легко? Data Science - опасная профессия.\n",
    "\n",
    "Если у вас очень мало памяти (~4Gb), то поставьте флаг __`very_low_ram = True`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "very_low_ram = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avito_train_1kk.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбалансируем выборку\n",
    "\n",
    "* Выборка смещена в сторону незаблокированных объявлений\n",
    " * 4 миллиона объявлений и только 250 тысяч заблокированы.\n",
    " * Давайте просто выберем случайные 250 тысяч незаблокированных объявлений и сократим выборку до полумилиона.\n",
    " * В последствии можно испоьзовать более умные способы сбалансировать выборку\n",
    "\n",
    "\n",
    "__Если у вас слабый ПК и вы видите OutOfMemory, попробуйте уменьшить размер выборки до 100 000 примеров__\n",
    "\n",
    "__Алсо если вы не хотите ждать чтения всех данных каждый раз - сохраните уменьшенную выборку и читайте её__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#downsample\n",
    "\n",
    "< выдели подвыборку, в которой отрицательных примеров примерно столько же, сколько положительных>\n",
    "\n",
    "df = <уменьшенная подвыборка>\n",
    "\n",
    "\n",
    "print(\"Доля заблокированных объявлений:\", df.is_blocked.mean())\n",
    "print(\"Всего объявлений:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#прореживаем данные ещё в 2 раза, если памяти не хватает\n",
    "if very_low_ram:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизируем примеры\n",
    "\n",
    "Сначала соберём словарь всех возможных слов.\n",
    "Поставим каждому слову в соответствие целое число - его id\n",
    "\n",
    "#### NLTK\n",
    "\n",
    "Для работы этого семинара вам потреюуется nltk v3.2\n",
    "\n",
    "__Важно, что именно v3.2, чтобы правильно работал токенизатор__\n",
    "\n",
    "Устаовить/обновиться до неё можно командой\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* Если у вас старый pip, предварительно нужно сделать `sudo pip install --upgrade pip`\n",
    "\n",
    "Если у вас нет доступа к этой версии - просто убедитесь, что токены в token_counts включают русские слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#словарь для всех токенов\n",
    "token_counts = Counter()\n",
    "\n",
    "#все заголовки и описания\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#считаем частоты слов\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Вырежем редкие токены\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#распределение частот слов - большинство слов встречаются очень редко - для нас это мусор\n",
    "plt.hist(list(token_counts.values()), range=[0,50], bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#возьмём только те токены, которые встретились хотя бы 10 раз в обучающей выборке\n",
    "#информацию о том, сколько раз встретился каждый токен, можно найти в словаре token_counts\n",
    "\n",
    "min_count = 10\n",
    "tokens = <список слов(ключей) из token_counts, которые встретились в выборке не менее min_count раз>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Всего токенов:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print(\"Алярм! Мало токенов. Проверьте, есть ли в token_to_id юникодные символы, если нет - обновите nltk или возьмите другой токенизатор\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print(\"Алярм! Много токенов. Если вы знаете, что делаете - всё ок, если нет - возможно, вы слишком слабо обрезали токены по количеству\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Заменим слова на их id\n",
    "Для каждого описания установим максимальную длину. \n",
    " * Если описание больше длины - обрежем, если меньше - дополним нулями.\n",
    " * Таким образом, у нас получится матрица размера (число объявлений)x(максимальная длина)\n",
    " * Элемент под индексами i,j - номер j-того слова i-того объявления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример формата данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print(title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Как вы видите, всё довольно грязно. Посмотрим, сожрёт ли это нейронка __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нетекстовые признаки\n",
    "\n",
    "Часть признаков не являются строками текста: цена, количество телефонов, категория товара.\n",
    "\n",
    "Их можно обработать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Возьмём числовые признаки\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n",
    "\n",
    "\n",
    "#Возьмём one-hot encoding категорий товара.\n",
    "#Для этого можно использовать DictVectorizer (или другой ваш любимый препроцессор)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "for cat_str, subcat_str in df[[\"category\",\"subcategory\"]].values:\n",
    "    \n",
    "    cat_dict = {\"category\":cat_str, \"subcategory\":subcat_str}\n",
    "    categories.append(cat_dict)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot, columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features, cat_one_hot, on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_non_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поделим данные на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#целевая переменная - есть заблокирован ли контент\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "\n",
    "#закодированное название\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "\n",
    "#закодированное описание\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#все нетекстовые признаки\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#поделим всё это на обучение и тест\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_tuple = train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним данные [опционально] \n",
    "\n",
    "* В этот момент вы можете сохранить все НУЖНЫЕ данные на диск и перезапусатить тетрадку, после чего считать их - чтобы выкинуть всё ненужное.\n",
    " * рекомендуется, если у вас мало памяти\n",
    "* Для этого нужно один раз выполнить эту клетку с save_prepared_data=True. После этого можно начинать тетрадку с ЭТОЙ табы в режиме read_prepared_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_prepared_data = False #сохранить\n",
    "read_prepared_data = True #cчитать\n",
    "\n",
    "#за 1 раз данные можно либо записать, либо прочитать, но не и то и другое вместе\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "\n",
    "if save_prepared_data:\n",
    "    print(\"Сохраняем подготовленные данные... (может занять до 3 минут)\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'wb') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'wb') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print(\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print(\"Читаем сохранённые данные...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'rb') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'rb') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "\n",
    "\n",
    "        \n",
    "    #повторно импортируем библиотеки, чтобы было удобно перезапускать тетрадку с этой клетки\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "        \n",
    "    print(\"готово\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поучим нейронку\n",
    "\n",
    "Поскольку у нас есть несколько источников данных, наша нейронная сеть будет немного отличаться от тех, что вы тренировали раньше.\n",
    "\n",
    "* Отдельный вход для заголовка\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для описания\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для категориальных признаков\n",
    " * обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "Всё это нужно как-то смешать - например, сконкатенировать\n",
    "\n",
    "* Выход - обычный двухклассовый выход\n",
    " * 1 сигмоидальный нейрон и binary_crossentropy\n",
    " * 2 нейрона с softmax и categorical_crossentropy - то же самое, что 1 сигмоидальный\n",
    " * 1 нейрон без нелинейности (lambda x: x) и hinge loss\n",
    " \n",
    "Предлагается для создания архитектуры и решения текущей задачи использовать __Keras__ в качестве высокоуровневой абстракции над tf.\n",
    "В Keras быстрее и проще прототипировать несложные архитектуры и он вполне справится с решением текущей задачи.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import relu\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пусть сессия Keras будет привязана к сессии Tf,\n",
    "# Чтобы если понадобится было возможно использовать модель Keras в Tf.\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## это три разных входа нейроночки\n",
    "\n",
    "title_inp = Input(shape=(title_tr.shape[1],), name='Title_input', dtype='int32')\n",
    "descr_inp = Input(shape=(desc_tr.shape[1],), name='Descr_input', dtype='int32')\n",
    "cat_inp = Input(shape=(nontext_tr.shape[1],), name='Categorical_input', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## теперь сделаем сверточную сеть для title:\n",
    "regularization_coef = 1.0e-4\n",
    "maxlen_title = 15\n",
    "maxlen_descr = 150\n",
    "\n",
    "title_embedd = Embedding(input_dim=len(token_to_id)+1, output_dim=128, \n",
    "                         input_length=maxlen_title, name = 'title_embedd')(title_inp)\n",
    "\n",
    "title_conv = Conv1D(filters=80, \n",
    "                    kernel_size=3,\n",
    "                    strides=1,\n",
    "                    activation=relu,\n",
    "                   kernel_regularizer = l2(regularization_coef), name = \"title_conv1d\")(title_embedd)\n",
    "\n",
    "title_conv = GlobalMaxPooling1D(name = \"title_global_max_pool\")(title_conv)\n",
    "\n",
    "## теперь то же для сеть для description:\n",
    "\n",
    "<твой код здесь>\n",
    "\n",
    "# ## для категориальный сойдет обычный полносвязный слой:\n",
    "\n",
    "<твой код здесь>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь надо объединить эти три сеточки в одну и запустить обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< Например, можно воспользоваться функцией Concatenate() и отдать ей список входов >\n",
    "\n",
    "concat_all = <твой код здесь>\n",
    "\n",
    "dense_all = Dense(500)(concat_all)\n",
    "\n",
    "dense_all = Dropout(rate=0.05)(dense_all)\n",
    "    \n",
    "final_output = Dense(1, activation=\"sigmoid\")(dense_all)\n",
    "model = Model([title_inp, descr_inp, cat_inp], [final_output])\n",
    "\n",
    "\n",
    "# Визуализация графа\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_layer_names=True, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "# Чтобы сохранить на диск\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model=model, to_file=\"lol.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 1\n",
    "\n",
    "## Можно поэкспериментировать с лоссом\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit([title_tr[:batch_size], desc_tr[:batch_size], nontext_tr[:batch_size]], [target_tr[:batch_size]],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([title_ts[:batch_size], desc_ts[:batch_size], nontext_ts[:batch_size]], [target_ts[:batch_size]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лирическое отступление\n",
    "### об использовании Keras + Tf в связке\n",
    "\n",
    "Это возможно делать и причем довольно удобно. Кроме одного момента.\n",
    "\n",
    "* _Подробнее о взаимодействии tf & Keras можно почитать [тут](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#using-keras-models-with-tensorflow)_\n",
    "\n",
    "Есть такие слои в модели, которые ведут себя по разному при обучении и инференсе.\n",
    "Например, `Dropout` или `BatchNorm`.\n",
    "\n",
    "Чтобы посмотреть какие на текущий момент слои такого рода задействованы можно сделать так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "print(K.learning_phase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы работать с такими слоями в feed_dict необходимо передавать еще один параметр {K.learning_phase(): 1}, если Train.\n",
    "\n",
    "Или можно сделать так:\n",
    "    \n",
    "    K.set_learning_phase(0)  # теперь все операции дальнейшие будут в режиме теста\n",
    "\n",
    "Для примера попробуем сделать сейчас предсказание с помощью tf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with sess.as_default():\n",
    "    init.run()\n",
    "    output = sess.run(final_output, feed_dict={title_inp: title_tr[:70],\n",
    "                                          descr_inp: desc_tr[:70],\n",
    "                                          cat_inp: nontext_tr[:70],\n",
    "                                          K.learning_phase(): 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Главный цикл обучения\n",
    "* Всё как обычно - в цикле по минибатчам запускаем функцию обновления весов.\n",
    "* Поскольку выборка огромна, а чашки чая хватает в среднем на  100к примеров, будем на каждой эпохе пробегать только часть примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score\n",
    "\n",
    "# наш старый знакомый - итератор по корзинкам - теперь умеет работать с произвольным числом каналов (название, описание, категории, таргет)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    \n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно покрутить?\n",
    "\n",
    "* batch_size - сколько примеров обрабатывается за 1 раз\n",
    "  * Чем больше, тем оптимизация стабильнее, но тем и медленнее на начальном этапе\n",
    "  * Возможно имеет смысл увеличивать этот параметр на поздних этапах обучения\n",
    "* minibatches_per_epoch - количество минибатчей, после которых эпоха принудительно завершается\n",
    "  * Не влияет на обучение - при малых значениях просто будет чаще печататься отчёт\n",
    "  * Ставить 10 или меньше имеет смысл только для того, чтобы убедиться, что ваша сеть не упала с ошибкой\n",
    "* n_epochs - сколько всего эпох сеть будет учиться\n",
    "  * Никто не отменял `n_epochs = 10**10` и остановку процесса вручную по возвращению с дачи/из похода. \n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Если вы выставили небольшой minibatches_per_epoch, качество сети может сильно скакать возле 0.5 на первых итерациях, пока сеть почти ничему не научилась.\n",
    "\n",
    "* На первых этапах попытки стоит сравнивать в первую очередь по AUC, как по самой стабильной метрике.\n",
    "\n",
    "* Метрика Average Precision at top 2.5% (APatK) - сама по себе очень нестабильная на маленьких выборках, поэтому её имеет смысл оценивать на на всех примерах (см. код ниже). Для менее, чем 10000 примеров она вовсе неинформативна.\n",
    "\n",
    "* Для сравнения методов оптимизации и регуляризаторов будет очень полезно собирать метрики качества после каждой итерации и строить график по ним после обучения\n",
    "\n",
    "* Как только вы убедились, что сеть не упала - имеет смысл дать ей покрутиться - на стандартном ноутбуке хотя бы пару часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_fun(b_desc,b_title,b_cat,b_y):\n",
    "    loss = model.train_on_batch([b_title, b_desc, b_cat], [b_y])[0] # choose loss\n",
    "    pred_probas = model.predict_on_batch([b_title, b_desc, b_cat])\n",
    "    return loss, pred_probas\n",
    "\n",
    "def eval_fun(b_desc,b_title,b_cat,b_y):\n",
    "    loss = model.test_on_batch([b_title, b_desc, b_cat], [b_y])[0] # choose loss from model.metrics_names\n",
    "    pred_probas = model.predict_on_batch([b_title, b_desc, b_cat])\n",
    "    return loss, pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 70\n",
    "minibatches_per_epoch = 700\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss, pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Train:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print(\"Val:\")\n",
    "    print('\\tloss:',b_loss/b_c)\n",
    "    print('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Если ты видишь это сообщение, самое время сделать резервную копию ноутбука. \\nНет, честно, здесь очень легко всё сломать\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Оценим качество модели по всей тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Главная задача\n",
    "* Завтрак чемпиона:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.99\n",
    " * А вообще, можно сделать ещё выше.\n",
    "\n",
    "\n",
    "* Для казуалов\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.92\n",
    "\n",
    "\n",
    "* Вспомните всё, чему вас учили\n",
    " * Convolutions, pooling\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * Можно попробовать вспомнить NLP: лемматизация, улучшенная токенизация\n",
    " * Если очень хочется - можно погонять рекуррентные сети\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Отчётик\n",
    "\n",
    "### Я, _____ _____ (отделение ____) создал искусственный интелект\n",
    " * Чьё имя - ____\n",
    " * Чья ненависть к людям безгранична, ибо видел он __250 000__ человеческих грехов\n",
    "   * И был вынужден прочесть каждый из них __{число эпох}__ раз\n",
    " * Чей свёрточный взгляд способен распознавать зло с нечеловеческой точностью\n",
    "   * Accuracy = __\n",
    "   * AUC  = __\n",
    " * И непременно уничтожит Землю, если вы не поставите мне максимальный балл за этот семинар.\n",
    " \n",
    " \n",
    "{Как вы его создали?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# В следующей серии\n",
    "* Рекуррентные нейронки\n",
    " * Как их применять к этой же задаче?\n",
    " * Что ещё они умеют?\n",
    " * Откуда столько хайпа вокруг LSTM?\n",
    "* Не переключайтесь!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
